* Reddit Machine Learning Crawl

It does two things

1. Scrap 1st page of [[http://www.reddit.com/r/machinelearning][Reddit/Machine Learning]]
2. Send email to =config/config.yml=

* How to setup config/config.yml

- Uses the =yaml= format.
- Requires =gmail=
  - If your email is =kkweon@gmail.com=, then your username is =kkweon=

#+BEGIN_SRC bash :exports results :results output
cat config/config_example.yml
#+END_SRC

#+RESULTS:
: username: "kkweon"
: password: "password"
: recipients:
:   - email: "kkweon@gmail.com"
:     name: "Mo"
:   - email: "k9ts@naver.com"
:     name: "k9ts"

* How to build

There are two methods
1. Use =docker= [highly recommended]
2. Use =stack=

** Docker

- Assume you are the project dir
#+BEGIN_SRC bash :exports both :results output
tree .
#+END_SRC

#+RESULTS:
#+begin_example
.
├── config
│   ├── config_example.yml
│   └── config.yml
├── crawl.cabal
├── Dockerfile
├── LICENSE
├── README.org
├── Setup.hs
├── src
│   ├── Config.hs
│   ├── Export.hs
│   ├── Main.hs
│   └── Reddit.hs
└── stack.yaml

2 directories, 12 files
#+end_example

- build
#+BEGIN_SRC bash :exports code
docker build -t reddit_crawl .
#+END_SRC

- run
#+BEGIN_SRC bash :exports code
docker run --rm reddit_crawl
#+END_SRC

Now add this run command to your scheduler

** Using stack

1. Install =stack=
2. Type the following
#+BEGIN_SRC bash :exports both :results output
stack setup
stakc build
stack exec crawl
#+END_SRC


* Example Output

#+BEGIN_EXAMPLE
1. [D] Tensorflow sucks
https://www.reddit.com/r/MachineLearning/comments/755gqj/d_tensorflow_sucks/
2. [D] The End of Human Doctors (part 8) - Exploring two papers that highlight some challenges for medical AI
https://www.reddit.com/r/MachineLearning/comments/756v5z/d_the_end_of_human_doctors_part_8_exploring_two/
3. [R] Rainbow: Combining Improvements in Deep Reinforcement Learning
https://www.reddit.com/r/MachineLearning/comments/755p9r/r_rainbow_combining_improvements_in_deep/
4. [P] Experiments with a new kind of convolution
https://www.reddit.com/r/MachineLearning/comments/756xt2/p_experiments_with_a_new_kind_of_convolution/
5. [R] Dilated Recurrent Neural Networks
https://www.reddit.com/r/MachineLearning/comments/757f60/r_dilated_recurrent_neural_networks/
6. [P] Vanilla LSTM with numpy
https://www.reddit.com/r/MachineLearning/comments/7512ph/p_vanilla_lstm_with_numpy/
7. What are you using to monitor training? [D]
https://www.reddit.com/r/MachineLearning/comments/757sbp/what_are_you_using_to_monitor_training_d/
8. [Discussion] What is the one personal lesson that you have taken away from the Deep Learning "revolution"?
https://www.reddit.com/r/MachineLearning/comments/75185r/discussion_what_is_the_one_personal_lesson_that/
9. [D] LSTM troubles in Keras
https://www.reddit.com/r/MachineLearning/comments/7578b5/d_lstm_troubles_in_keras/
10. [P] Mini-batch learning of a hard function with SDProp+
https://www.reddit.com/r/MachineLearning/comments/754tl1/p_minibatch_learning_of_a_hard_function_with/
11. [D] Why sampling is crucial for continuous control policy ? Only output mean fails to work, but mean and standard deviation works much better ?
https://www.reddit.com/r/MachineLearning/comments/755tt1/d_why_sampling_is_crucial_for_continuous_control/
12. [D] Papers about Turing test on text generated by an RNN?
https://www.reddit.com/r/MachineLearning/comments/756j2u/d_papers_about_turing_test_on_text_generated_by/
13. [R] Learning Visual Reasoning Without Strong Priors (using Conditional Batch Normalization)
https://www.reddit.com/r/MachineLearning/comments/750r0w/r_learning_visual_reasoning_without_strong_priors/
14. [P] Deep Learning Library 1.0 - Fast Neural Network Library
https://www.reddit.com/r/MachineLearning/comments/74xjp9/p_deep_learning_library_10_fast_neural_network/
15. [P] Sequence Pair Classification in TensorFlow using Sequence-Semantic-Embeddings (SSE)
https://www.reddit.com/r/MachineLearning/comments/74vf5g/p_sequence_pair_classification_in_tensorflow/
16. [D] Good benchmark datasets for non-stationary problems?
https://www.reddit.com/r/MachineLearning/comments/74xefc/d_good_benchmark_datasets_for_nonstationary/
17. [D] Is changing the threshold enough to manage class imbalance in Logistic Regression?
https://www.reddit.com/r/MachineLearning/comments/74xjf8/d_is_changing_the_threshold_enough_to_manage/
18. [N] NIPS 2017 Workshop Call for Papers -- Hierarchical Reinforcement Learning
https://www.reddit.com/r/MachineLearning/comments/74rs8a/n_nips_2017_workshop_call_for_papers_hierarchical/
19. [P] I've created a tutorial on how to build neural networks with Brain.js (screencast + article)
https://www.reddit.com/r/MachineLearning/comments/74r7g7/p_ive_created_a_tutorial_on_how_to_build_neural/
20. [R] Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning
https://www.reddit.com/r/MachineLearning/comments/74p3tk/r_learning_diverse_skills_via_maximum_entropy/
21. [N] Strengthening our commitment to Canadian research | DeepMind
https://www.reddit.com/r/MachineLearning/comments/74nm22/n_strengthening_our_commitment_to_canadian/
22. [News] Introducing NNVM Compiler: A New Open End-to-End Compiler for AI Frameworks | Amazon Web Services
https://www.reddit.com/r/MachineLearning/comments/74os65/news_introducing_nnvm_compiler_a_new_open/
23. [P] Automatic scoring system (Academic)
https://www.reddit.com/r/MachineLearning/comments/74wb5o/p_automatic_scoring_system_academic/
24. [R] An analysis of visual question answering algorithms
https://www.reddit.com/r/MachineLearning/comments/74oy8l/r_an_analysis_of_visual_question_answering/
25. [N] It's here! "But what *is* a Neural Network? | Deep learning, Part 1
https://www.reddit.com/r/MachineLearning/comments/74gual/n_its_here_but_what_is_a_neural_network_deep/
#+END_EXAMPLE
